{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aiquanhuy/clipcap-with-flickr8k?scriptVersionId=103852599\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Download and preprocess dataset: flickr8k","metadata":{"id":"Px7KnkTEPtfN"}},{"cell_type":"code","source":"import glob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom time import time\nfrom numpy import array","metadata":{"id":"v-jDQ7ISyk_6","execution":{"iopub.status.busy":"2022-08-21T14:09:55.468967Z","iopub.execute_input":"2022-08-21T14:09:55.470695Z","iopub.status.idle":"2022-08-21T14:09:55.485756Z","shell.execute_reply.started":"2022-08-21T14:09:55.470634Z","shell.execute_reply":"2022-08-21T14:09:55.483936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/flickr8k/captions.txt\")\ndf","metadata":{"id":"aRgupW-nyKb7","outputId":"e6276616-da9e-4fca-8ba6-31a851f68962","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import re\n# def caption_preprocessing(text, remove_digits=True):\n#   pattern=r'[^a-zA-z0-9\\s]'\n#   text=re.sub(pattern,'',text)\n#   # tokenize\n#   text=text.split()\n#   # convert to lower case\n#   text = [word.lower() for word in text]\n#   # remove hanging 's' and 'a'\n#   # text = [word for word in text if len(word)>1]\n  \n#   # remove tokens with numbers in them\n#   text = [word for word in text if word.isalpha()]\n#   # store as string\n#   text =  ' '.join(text)\n\n#   # insert 'startseq', 'endseq' cho chuá»—i\n#   text = 'startseq ' + text + ' endseq'\n#   return text\n\n# print(caption_preprocessing('chao .. ban $ hello980 it\\'s a table.#'))","metadata":{"id":"EaqyXH3s0T0N","outputId":"41d7dc37-737b-43bb-a2ce-9303b05476bb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['caption'] = df['caption'].apply(caption_preprocessing)\n# df['caption']","metadata":{"id":"J-KPJAwC0Tx5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"yhMV80IH0Tvx","outputId":"af3672bd-dc6e-4756-cf13-3aba942c91a6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# $ conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n! pip install ftfy regex tqdm\n! pip install git+https://github.com/openai/CLIP.git","metadata":{"id":"EFtYDUXI9_de","outputId":"cfbee85e-213b-47b4-e03b-70db09df040c","execution":{"iopub.status.busy":"2022-08-21T14:10:16.09754Z","iopub.execute_input":"2022-08-21T14:10:16.09812Z","iopub.status.idle":"2022-08-21T14:10:50.88979Z","shell.execute_reply.started":"2022-08-21T14:10:16.098075Z","shell.execute_reply":"2022-08-21T14:10:50.888047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport skimage.io as io\nimport clip\nfrom PIL import Image\nimport pickle\nimport json\nimport os\nfrom tqdm import tqdm\nimport argparse","metadata":{"id":"RXh8smZzyvgP","execution":{"iopub.status.busy":"2022-08-21T14:11:11.839855Z","iopub.execute_input":"2022-08-21T14:11:11.841075Z","iopub.status.idle":"2022-08-21T14:11:11.848728Z","shell.execute_reply.started":"2022-08-21T14:11:11.84104Z","shell.execute_reply":"2022-08-21T14:11:11.846607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse(clip_model_type: str):\n    device = torch.device('cuda:0')\n    clip_model_name = clip_model_type.replace('/', '_')\n    out_path = f\"./oscar_split_{clip_model_name}_train.pkl\"\n    clip_model, preprocess = clip.load(clip_model_type, device=device, jit=False)\n    all_embeddings = []\n    all_captions = []\n    for i in tqdm(range(len(df))):\n        d=df.iloc[i]\n        img_id = d[\"image\"]\n        filename = f\"../input/flickr8k/Images/{img_id}\"\n        image = Image.open(filename).convert(\"RGB\")\n        image = preprocess(image).unsqueeze(0).to(device)\n        with torch.no_grad():\n            prefix = clip_model.encode_image(image).cpu()\n        d[\"clip_embedding\"] = i\n        all_embeddings.append(prefix)\n        all_captions.append(d)\n        if (i + 1) % 10000 == 0:\n            with open(out_path, 'wb') as f:\n                pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n\n    with open(out_path, 'wb') as f:\n        pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n\n    print('Done')\n    print(\"%0d embeddings saved \" % len(all_embeddings))\n    return 0\n","metadata":{"id":"FF98n4HEm9Q-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip.available_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parse('RN50x4')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import clip\nimport os\nfrom torch import nn\nimport numpy as np\nimport torch\nimport torch.nn.functional as nnf\nimport sys\nfrom typing import Tuple, List, Union, Optional\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm, trange\n# from google.colab import files\nimport skimage.io as io\nimport PIL.Image\n# from IPython.display import Image \nfrom enum import Enum\n","metadata":{"execution":{"iopub.status.busy":"2022-08-21T14:11:16.648493Z","iopub.execute_input":"2022-08-21T14:11:16.648924Z","iopub.status.idle":"2022-08-21T14:11:16.66127Z","shell.execute_reply.started":"2022-08-21T14:11:16.64889Z","shell.execute_reply":"2022-08-21T14:11:16.659264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as nnf\nfrom torch.utils.data import Dataset, DataLoader\nfrom enum import Enum\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nimport os\nimport pickle\nimport sys\nimport argparse\nimport json\nfrom typing import Tuple, Optional, Union\n","metadata":{"execution":{"iopub.status.busy":"2022-08-21T14:11:18.311403Z","iopub.execute_input":"2022-08-21T14:11:18.312187Z","iopub.status.idle":"2022-08-21T14:11:18.33579Z","shell.execute_reply.started":"2022-08-21T14:11:18.312122Z","shell.execute_reply":"2022-08-21T14:11:18.334179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClipCocoDataset(Dataset):\n\n    def __len__(self) -> int:\n        return len(self.captions_tokens)\n\n    def pad_tokens(self, item: int):\n        tokens = self.captions_tokens[item]\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            self.captions_tokens[item] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]\n            self.captions_tokens[item] = tokens\n        mask = tokens.ge(0)  # mask is zero where we out of sequence\n        tokens[~mask] = 0\n        mask = mask.float()\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n        return tokens, mask\n\n    def __getitem__(self, item: int) -> Tuple[torch.Tensor, ...]:\n        tokens, mask = self.pad_tokens(item)\n        prefix = self.prefixes[self.caption2embedding[item]]\n        if self.normalize_prefix:\n            prefix = prefix.float()\n            prefix = prefix / prefix.norm(2, -1)\n        return tokens, mask, prefix\n\n    def __init__(self, data_path: str,  prefix_length: int, gpt2_type: str = \"gpt2\",\n                 normalize_prefix=False):\n        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n        with open(data_path, 'rb') as f:\n            all_data = pickle.load(f)\n        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n        sys.stdout.flush()\n        self.prefixes = all_data[\"clip_embedding\"]\n        captions_raw = all_data[\"captions\"]\n        self.image_ids = [caption[\"image\"] for caption in captions_raw]\n        self.captions = [caption['caption'] for caption in captions_raw]\n        if os.path.isfile(f\"{data_path[:-4]}_tokens.pkl\"):\n            with open(f\"{data_path[:-4]}_tokens.pkl\", 'rb') as f:\n                self.captions_tokens, self.caption2embedding, self.max_seq_len = pickle.load(f)\n        else:\n            self.captions_tokens = []\n            self.caption2embedding = []\n            max_seq_len = 0\n            for caption in captions_raw:\n                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption['caption']), dtype=torch.int64))\n                self.caption2embedding.append(caption[\"clip_embedding\"])\n                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n            # self.max_seq_len = max_seq_len\n            with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n                pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Model\n\n\nclass MappingType(Enum):\n    MLP = 'mlp'\n    Transformer = 'transformer'\n\n\nclass MlpTransformer(nn.Module):\n     def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):\n         super().__init__()\n         out_d = out_d if out_d is not None else in_dim\n         self.fc1 = nn.Linear(in_dim, h_dim)\n         self.act = act\n         self.fc2 = nn.Linear(h_dim, out_d)\n         self.dropout = nn.Dropout(dropout)\n\n     def forward(self, x):\n         x = self.fc1(x)\n         x = self.act(x)\n         x = self.dropout(x)\n         x = self.fc2(x)\n         x = self.dropout(x)\n         return x\n\nclass MLP(nn.Module):\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x)\n\n    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n        super(MLP, self).__init__()\n        layers = []\n        for i in range(len(sizes) - 1):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n            if i < len(sizes) - 2:\n                layers.append(act())\n        self.model = nn.Sequential(*layers)\n\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim_self // num_heads\n        self.scale = head_dim ** -0.5\n        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n        self.project = nn.Linear(dim_self, dim_self)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, y=None, mask=None):\n        y = y if y is not None else x\n        b, n, c = x.shape\n        _, m, d = y.shape\n        # b n h dh\n        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n        # b m 2 h dh\n        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n        if mask is not None:\n            if mask.dim() == 2:\n                mask = mask.unsqueeze(1)\n            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n        attention = attention.softmax(dim=2)\n        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n        out = self.project(out)\n        return out, attention\n\n\nclass TransformerLayer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        x_, attention = self.attn(self.norm1(x), y, mask)\n        x = x + x_\n        x = x + self.mlp(self.norm2(x))\n        return x, attention\n\n    def forward(self, x, y=None, mask=None):\n        x = x + self.attn(self.norm1(x), y, mask)[0]\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n                 norm_layer: nn.Module = nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim_self)\n        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n        self.norm2 = norm_layer(dim_self)\n        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n\n\nclass Transformer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        attentions = []\n        for layer in self.layers:\n            x, att = layer.forward_with_attention(x, y, mask)\n            attentions.append(att)\n        return x, attentions\n\n    def forward(self, x, y=None, mask=None):\n        for i, layer in enumerate(self.layers):\n            if i % 2 == 0 and self.enc_dec: # cross\n                x = layer(x, y)\n            elif self.enc_dec:  # self\n                x = layer(x, x, mask)\n            else:  # self or cross\n                x = layer(x, y, mask)\n        return x\n\n    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n        super(Transformer, self).__init__()\n        dim_ref = dim_ref if dim_ref is not None else dim_self\n        self.enc_dec = enc_dec\n        if enc_dec:\n            num_layers = num_layers * 2\n        layers = []\n        for i in range(num_layers):\n            if i % 2 == 0 and enc_dec:  # cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            elif enc_dec:  # self\n                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            else:  # self or cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n        self.layers = nn.ModuleList(layers)\n\n\nclass TransformerMapper(nn.Module):\n\n    def forward(self, x):\n        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n        prefix = torch.cat((x, prefix), dim=1)\n        out = self.transformer(prefix)[:, self.clip_length:]\n        return out\n\n    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n        super(TransformerMapper, self).__init__()\n        self.clip_length = clip_length\n        self.transformer = Transformer(dim_embedding, 8, num_layers)\n        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)\n\n\nclass ClipCaptionModel(nn.Module):\n\n    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n\n    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n                labels: Optional[torch.Tensor] = None):\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n        return out\n\n    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n        if mapping_type == MappingType.MLP:\n            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n                                     self.gpt_embedding_size * prefix_length))\n        else:\n            self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n                                                                     clip_length, num_layers)\n\n\nclass ClipCaptionPrefix(ClipCaptionModel):\n\n    def parameters(self, recurse: bool = True):\n        return self.clip_project.parameters()\n\n    def train(self, mode: bool = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self\n    \n\n\n","metadata":{"id":"_LyQwFeSOqUG","execution":{"iopub.status.busy":"2022-08-21T14:11:25.20496Z","iopub.execute_input":"2022-08-21T14:11:25.205444Z","iopub.status.idle":"2022-08-21T14:11:25.257256Z","shell.execute_reply.started":"2022-08-21T14:11:25.205409Z","shell.execute_reply":"2022-08-21T14:11:25.255194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(dataset: ClipCocoDataset, model: ClipCaptionModel, \n          bs=64,epochs=10,save_every=5,\n          lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"coco_prefix\"):\n\n    device = torch.device('cuda:0')\n    batch_size = bs\n    epochs = epochs\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    model = model.to(device)\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=lr)\n    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n    )\n    # save_config(args)\n    for epoch in range(epochs):\n        print(f\">>> Training epoch {epoch}\")\n        sys.stdout.flush()\n        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n            model.zero_grad()\n            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n            outputs = model(tokens, prefix, mask)\n            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            progress.set_postfix({\"loss\": loss.item()})\n            progress.update()\n            if (idx + 1) % 10000 == 0:\n                torch.save(\n                    model.state_dict(),\n                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n                )\n        progress.close()\n        if epoch % save_every == 0 or epoch == epochs - 1:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n            )\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prefix_length = 40\nprefix_dim = 640 \nmapping_type = MappingType.Transformer\ndata_path= './oscar_split_RN50x4_train.pkl'\nonly_prefix = True\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = ClipCocoDataset(data_path, prefix_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=ClipCaptionPrefix(40, clip_length=40, prefix_size=640,\n                        num_layers=8, mapping_type='transformer')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for param_tensor in model.state_dict():\n#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(dataset,model,\n          bs=64,epochs=20,save_every=5,\n          lr= 2e-5, warmup_steps = 5000, output_dir = \"./\", output_prefix = \"coco_prefix\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# predict","metadata":{}},{"cell_type":"code","source":"#@title Caption prediction\n\ndef generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n                  entry_length=67, temperature=1., stop_token: str = '.'):\n\n    model.eval()\n    stop_token_index = tokenizer.encode(stop_token)[0]\n    tokens = None\n    scores = None\n    device = next(model.parameters()).device\n    seq_lengths = torch.ones(beam_size, device=device)\n    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n    with torch.no_grad():\n        if embed is not None:\n            generated = embed\n        else:\n            if tokens is None:\n                tokens = torch.tensor(tokenizer.encode(prompt))\n                tokens = tokens.unsqueeze(0).to(device)\n                generated = model.gpt.transformer.wte(tokens)\n        for i in range(entry_length):\n            outputs = model.gpt(inputs_embeds=generated)\n            logits = outputs.logits\n            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n            logits = logits.softmax(-1).log()\n            if scores is None:\n                scores, next_tokens = logits.topk(beam_size, -1)\n                generated = generated.expand(beam_size, *generated.shape[1:])\n                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n                if tokens is None:\n                    tokens = next_tokens\n                else:\n                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n                    tokens = torch.cat((tokens, next_tokens), dim=1)\n            else:\n                logits[is_stopped] = -float(np.inf)\n                logits[is_stopped, 0] = 0\n                scores_sum = scores[:, None] + logits\n                seq_lengths[~is_stopped] += 1\n                scores_sum_average = scores_sum / seq_lengths[:, None]\n                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n                next_tokens_source = next_tokens // scores_sum.shape[1]\n                seq_lengths = seq_lengths[next_tokens_source]\n                next_tokens = next_tokens % scores_sum.shape[1]\n                next_tokens = next_tokens.unsqueeze(1)\n                tokens = tokens[next_tokens_source]\n                tokens = torch.cat((tokens, next_tokens), dim=1)\n                generated = generated[next_tokens_source]\n                scores = scores_sum_average * seq_lengths\n                is_stopped = is_stopped[next_tokens_source]\n            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n            generated = torch.cat((generated, next_token_embed), dim=1)\n            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n            if is_stopped.all():\n                break\n    scores = scores / seq_lengths\n    output_list = tokens.cpu().numpy()\n    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n    order = scores.argsort(descending=True)\n    output_texts = [output_texts[i] for i in order]\n    return output_texts\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-21T14:11:34.945653Z","iopub.execute_input":"2022-08-21T14:11:34.946167Z","iopub.status.idle":"2022-08-21T14:11:34.977626Z","shell.execute_reply.started":"2022-08-21T14:11:34.946114Z","shell.execute_reply":"2022-08-21T14:11:34.975908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0')\nclip_model, preprocess = clip.load(\"RN50x4\", device=device, jit=False)\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2022-08-21T14:12:04.111321Z","iopub.execute_input":"2022-08-21T14:12:04.111812Z","iopub.status.idle":"2022-08-21T14:12:41.545687Z","shell.execute_reply.started":"2022-08-21T14:12:04.111774Z","shell.execute_reply":"2022-08-21T14:12:41.54424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prefix_length = 40\n\nmodel = ClipCaptionPrefix(prefix_length, clip_length=40, prefix_size=640,\n                                  num_layers=8, mapping_type='transformer')\nmodel.load_state_dict(torch.load('../input/flickr8k20epoch/coco_prefix-019.pt', map_location='cpu')) \n\nmodel = model.eval() \n# device = CUDA(0) if is_gpu else \"cpu\"\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T14:49:22.740206Z","iopub.execute_input":"2022-08-21T14:49:22.740739Z","iopub.status.idle":"2022-08-21T14:49:27.477368Z","shell.execute_reply.started":"2022-08-21T14:49:22.740681Z","shell.execute_reply":"2022-08-21T14:49:27.47593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = Image.open('../input/coco-2017-dataset/coco2017/test2017/000000000647.jpg').convert(\"RGB\")\nimage = preprocess(images).unsqueeze(0).to(device)\ndisplay(images)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T14:51:40.52779Z","iopub.execute_input":"2022-08-21T14:51:40.528222Z","iopub.status.idle":"2022-08-21T14:51:40.686156Z","shell.execute_reply.started":"2022-08-21T14:51:40.528188Z","shell.execute_reply":"2022-08-21T14:51:40.684878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_beam_search = True #@param {type:\"boolean\"}  \n\n# image = io.imread(UPLOADED_FILE)\n# pil_image = PIL.Image.fromarray(image)\n# #pil_img = Image(filename=UPLOADED_FILE)\n# display(pil_image)\n\n# image = preprocess(pil_image).unsqueeze(0).to(device)\nwith torch.no_grad():\n    prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n    prefix = prefix / prefix.norm(2, -1).item()\n    prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\nif use_beam_search:\n    generated_text_prefix = generate_beam(model, tokenizer, beam_size=7,embed=prefix_embed,stop_token='<|endoftext|>',entry_length=30)[0]\n\n\nprint('\\n')\nprint(generated_text_prefix)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T14:51:54.854788Z","iopub.execute_input":"2022-08-21T14:51:54.855345Z","iopub.status.idle":"2022-08-21T14:51:55.667671Z","shell.execute_reply.started":"2022-08-21T14:51:54.855307Z","shell.execute_reply":"2022-08-21T14:51:55.665619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}